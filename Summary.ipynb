{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Summary.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNJkJ7mFx0DIeOd6kSecOkS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabiolafaria/ET5003-Etivity/blob/main/Summary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PB1LkERX98kb"
      },
      "source": [
        "# SUMMARY\n",
        "\n",
        "In this etivity the main objective was to perform supervised learning on a house price dataset and contrast a single Bayesian Linear Regression model with a Piecewise Bayesian Linear Regression model.  \n",
        "\n",
        "The house dataset that we worked through this etivity were compounded by the following features initially, where we performed exploratory data analysis for cleaning the dataset: \n",
        "\n",
        "`ad_id`, `area`, `bathrooms`, `beds`, `ber_classification`, `county`,`description_block`, `environment`, `facility`, `features`, `latitude`,`longitude`, `no_of_units`, `property_category`, `property_type`, `surface`, `price`\n",
        "\n",
        "**Exploratory Data Analysis**\n",
        "\n",
        "The exploratory data analysis was due to clean some features with invalid or missing values data and outliers were removed from our training dataset as we noticed that missing values for `beds`, `bathrooms` and `property_type` were coincident. With our heatmap, we were able to see a positive linear correlation between the remaining relevant features beds, bathrooms and price, followed by longitude and latitude, although the latitude with a negative correlation.\n",
        "\n",
        "For the full baseline model, we used `beds`, `latitude`, `longitude`, `surface`, `price` as per the exploratory analysis we found some positive and negative relationships to price.\n",
        " \n",
        "**Fitting the Model**\n",
        "\n",
        "While using different features scales in our baseline model, we applied feature scaling with StandardScaler in the training dataset to normalize these attributes. When building our basic model, it creates a random variable with Normal prior distributions for the regression coefficients with a mean of 0, standard deviation of 30. We used the mean absolute error (MAE) and mean percentage error (MAPE) metrics to evaluate the performance of model prediction where high values for loss may represent a poor performance.\n",
        "  \n",
        "Following data cleaning we trained a Bayesian Linear model on the full training dataset. We calculated the MAE and MAPE scores to allow for later comparison against the piecewise linear models. \n",
        "\n",
        "**Clustering**\n",
        "  \n",
        "For piecewise linear regression we used multiple models which are trained on a subset of the data. Switchpoints determine when we will use which model. We decide the switchpoints in our case by clustering the training data with a Gaussian Mixture Model (GMM). Subsequent to fitting the GMM on our training data we will be able to predict using the GMM which cluster our test data or new data belongs to. After training each piecewise model on a single cluster of data we have individual models which best represent a cluster of the dataset. The intuition behind piecewise linear models is that the individual models can better represent the data than a single linear model.  \n",
        "  \n",
        "We determined through the use of k-means clustering with elbow and silhouette methods that the training data had 5 clusters. Initially the methods showed 4 and 9 clusters for elbow and silhouette methods but after further data cleaning which removed invalid entries, replaced 0 **beds** with the mode and excluded some outliers in the dataset this became 5 and 7. We trialed clustering on discrete variables, **beds**, and also on continuous variables, {**lat**, **long**}, before deciding to use **lat** and **long** as we thought the continous variables would give more granularity to the model to fit the data than the discrete ones. The cluster sizes seemed reasonable although one is a multiple in size compared to the others. \n",
        "  \n",
        "We then trained our Bayesian piecewise linear regression models with each linear regression model represented by the following where $i=[0, 4]$:  \n",
        "$$y(x) = \\alpha_i+\\beta_i^T X $$  \n",
        "\n",
        "**Simulations**\n",
        "\n",
        "With posterior predictive checks (PPCs) we were able to validate the model predictions by clusters against the actual observations as presented in our plots. We can see in these plots the degree to which predictions models can deviate from data generated from the true observations. Cluster 3 model followed by Cluster 0 model presented bigger deviation compared to the remaining Clusters. Overall we can observe some prominent deviation in our combining Cluster on the training set, even though our data has been normalized.\n",
        "\n",
        "**Discussion and Conclusion**\n",
        "\n",
        "On determining the MAE and MAPE scores for the joint linear model we see that there is approximately a 2% drop in MAPE between the full and piecewise model joint result. The MAE loss score is lower in the piecewise model. These scores show the piecewise model outperforms the single model for this dataset. Meanwhile, based on the performance result we can see the MAE and MAPE result for each cluster are better than the joint result. It proved the house price followed different linear trends over different regions of the data.\n",
        "\n",
        "| Model        | MAE           | MAPE  |\n",
        "| :------------- |:-------------:| -----:|\n",
        "| Full      | 212380 | 0.282 |\n",
        "| Piecewise Joint      | 200342      |  0.2637 |  \n",
        "| Piecewise Cluster 0      | 265496      |  0.2618 |  \n",
        "| Piecewise Cluster 1      | 84487       |  0.2146 |  \n",
        "| Piecewise Cluster 2      | 214326      |  0.2764 |  \n",
        "| Piecewise Cluster 3      | 154579      |  0.2455 | \n",
        "| Piecewise Cluster 4      | 167491      |  0.2467 |\n",
        "  \n",
        "Initially we fit the GMM clustering on both the training and test datasets as per the lab notebook. However we believe this mis-represents the performance of the piecewise linear model as it must include the clustering as a pre-step to determine which of the individual models to use when it is used on out-of-sample data. For completeness the performance seen when the GMM was trained with both training and test data was:\n",
        "\n",
        "| Model        | MAE           | MAPE  |\n",
        "| :------------- |:-------------:| -----:|\n",
        "| Full      | 212380 | 0.282 |\n",
        "| Piecewise      | 200342      |  0.2637 |\n",
        "| Piecewise (GMM fit on test and training)      | 199767      |  0.244 |    \n",
        "  \n",
        "We believe a better result could probably be attained by doing further data cleaning. Our experience was that both the full and piecewise model losses decreased as we performed further data cleaning and perhaps making better use of the categorical variables. Overall piecewise linear regression is shown to be a useful tool.    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0GYCpwEM09T"
      },
      "source": [
        "# References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kdk0defr98kb"
      },
      "source": [
        "Rousseeuw, P. J. (1987). Silhouettes: A graphical aid to the interpretation and validation of cluster analysis. Journal of Computational and Applied Mathematics, 20(C), 53â€“65. https://doi.org/10.1016/0377-0427(87)90125-7\n",
        "\n",
        "Seif, G. (2019) Understanding the 3 most common loss functions for Machine Learning Regression. Available at https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3 (Accessed: 24 September 2021).\n",
        "  \n",
        "Solanki, G., 2020. Understanding Gaussian Mixture Model, Available at: https://www.mygreatlearning.com/blog/gaussian-mixture-model/ (Accessed: 28 September 2021).\n",
        "  \n",
        "Statistics How To (2021), Mean Absolute Percentage Error (MAPE). Available at https://www.statisticshowto.com/mean-absolute-percentage-error-mape/ (Accessed: 2 October 2021).\n",
        "  \n",
        "Statistics How To (2021), Mean Absolute Error (MAE). Available at \n",
        "https://www.statisticshowto.com/absolute-error/ (Accessed: 2 October 2021)."
      ]
    }
  ]
}